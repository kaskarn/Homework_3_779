---
title: 'Homework #3'
subtitle: 'Analysis of Arsenic in Rice Products'
author: "Antoine Baldassari"
date: "November 17, 2015"
output: 
  pdf_document:
    highlight: zenburn
    keep_tex: yes
---

\section{1. Standard conditionally-conjugate specification of the hierarchical model}
\subsection{1.1 Model specification}

In this model specification, the $i^{th}$ arsenic reading of group $j$, $y_{ij}$ is normally-distributed, so that 
$$y_{ij} \sim \mathcal{N}\left( \theta_j, \sigma^2 \right)$$
Where $\theta_j$ is the mean arsenic reading for the rice products indexed by $j$. $\theta_j$ is normally distributed, centered at the population mean $\mu$, with between-group variance $\tau^2$:
$$\theta_j \sim \mathcal{N}\left( \mu, \tau^2 \right) $$
We use conditionally-conjugate Normal and Inverse-Gamma priors on the hyperparameters:
	\begin{align*}
		1/\sigma^2 & \sim \text{gamma }(\nu_0/2, \nu_0\sigma^2_0/2)\\
		1/\tau^2 & \sim \text{gamma }(\eta_0/2,\eta_0\tau_0^2/2)\\
		\mu & \sim \text{normal }(\mu_0, \gamma_0^2)
	\end{align*}
	The full conditional distribution of the parameters can be found to be (from the book):
	\begin{align*}
		\left\{ \theta_j \, | \, \sigma^2, y_{j,1},\ldots,y_{j,n} \right\} \sim& \, \mathcal{N}\left( \frac{n_j\bar{y}_j/\sigma^2+\mu/\tau^2}{n_j/\sigma^2 + 1/\tau^2}, \left[ n_j/\sigma^2+1/\tau^2\right]^{-1} \right) \\
		\left\{\mu \, | \, \theta_1,\ldots,\theta_m,\tau \right\} \sim& \, \mathcal{N} \left( \frac{m\bar{\theta}/\tau^2 + \mu_0/\gamma_0^2}{m/\tau^2+1/\gamma_0^2},\left[ m/\tau^2 + 1/\gamma_0^2\right]^{-1} \right)\\
		\left\{ 1/\tau^2 \, | \, \theta_1, \ldots, \theta_m, \mu \right\} \sim& \, \text{gamma}\left( \frac{\eta_0 +m}{2},\frac{\eta_o\tau^2_0+\sum\left(\theta_j -\mu \right)^2}{2} \right) \\
		\left\{1/\sigma^2 \, | \, \boldsymbol{\theta}, y_1, \ldots, y_n \right\} \sim& \, \text{gamma}\left( \frac{1}{2}\left[ \nu_0 + \sum\limits_{j=1}^m n_j \right], \frac{1}{2}\left(\nu_0\sigma_0^2 + \sum\limits_{j=1}^m \sum\limits_{i=1}^{n_j} \left(y_{i,j} -\theta_j \right)^2 \right)\right)
	\end{align*}

\subsection{1.2 Main analyses}
We pick relatively uninformative priors, centering $\mu$ around $1$ with somewhat large within and between sample variances: $\sigma^2_0 = 10, \nu_0 = 1, \tau_0^2 = 10, \eta_0=1, \gamma_0^2 = 10$.
The marginal distributions of $\theta_1, \ldots, \theta_m, \mu, \sigma^2$ and $\tau^2$ can be obtained from the full condition distributions using a Monte-Carlo Markov-Chain algorithm, Gibbs sampling, which we implement in R as follows::\newline


First, we input the dataset downloaded from Sakai, modified in Stata to have numeric codes for rice products categories.
```{.r}
library(foreign)
Y <- read.dta(file="arsenicrice2.dta")
```

We set the weakly informative prior values
```{.r}
n <-  nrow(Y)
nu0 <- 1; eta0 <- 1; 
t20 <- 10;
mu0 <- 1; 
g20 <- s20 <- var(Y$arsenic)
```	

We set initial values for algorithm 
```{.r}
m <- length(unique(Y$food_num)) #number of groups
n <- sv <- ybar <- rep(NA,m) 
for (i in 1:m) 
{
  n[i] <- sum(Y$food_num==i)
  sv[i] <- var(Y$arsenic[which(Y$food_num==i)])
  ybar[i] <- mean(Y$arsenic[which(Y$food_num==i)])
}
theta <- ybar; s2 <- mean(sv)
mu <- mean(theta); tau2 <- var(theta)
```	

```{.r, echo=FALSE}
newTheta <- function(n, ybar, s2, tau2, mu)
{
  v = 1/(n/s2 +1/tau2)
  e = v * (ybar*n/s2 +mu/tau2)
  new <- rnorm(1, e, sqrt(v))
  return(new)
}
newSigma2 <- function(m, n, nu0, s20, theta, Y)
{
  nun = nu0 + sum(n)
  ss <- nu0 * s20
  for(i in 1:m) ss = ss+sum((Y$arsenic[which(Y$food_num==i)] - theta[j])^2)
  sigma2 <- 1/rgamma(1, nun/2, ss/2)
  return(sigma2)
}
newMu <- function(m, theta, tau2, g20)
{
  v = 1/(m/tau2 + 1/g20)
  e = v *(m*mean(theta)/tau2 + mu0/g20)
  mu <- rnorm(1, e, v)
  return(mu)
}
newTau2 <- function(m, eta0, t20, theta, mu)
{
  etam = eta0 + m
  ss <- eta0*t20 + sum( (theta-mu) ^2 )
  tau2 <- 1/rgamma(1, etam/2, ss/2)
  return(tau2)
}
```

We create a Markov chain for each parameter by sequentially sampling from their posterior over 10,000 iterations. Elements are stored in the chain at the end of each iteration. 
```{.r}
#Setup MCMC
set.seed(0808)
S <- 10000
THETA <- matrix(nrow=S, ncol=m)
OTH <- matrix(nrow=S, ncol=3)
ALL <- matrix(nrow=S, ncol=3+m)

#Run algorithm
for(i in 1:S)
{
  #Get new values for parameters
  for(j in 1:m) theta[j] <- newTheta(n[j], ybar[j], s2, tau2, mu)
  s2 <- newSigma2(m, n, nu0, s20, theta, Y)
  mu <- newMu(m, theta, tau2, g20)
  tau2 <- newTau2(m, eta0, t20, theta, mu)
  
  #Store in chain
  THETA[i,] <- theta
  OTH[i,] <- c(mu,s2,tau2)
  ALL[i,] <- c(theta,mu,s2,tau2)
}
```
Where the functions updating the parameters follow the equations listed above:
```{.r}
newTheta <- function(n, ybar, s2, tau2, mu)
{
  v = 1/(n/s2 +1/tau2)
  e = v * (ybar*n/s2 +mu/tau2)
  new <- rnorm(1, e, sqrt(v))
  return(new)
}
newSigma2 <- function(m, n, nu0, s20, theta, Y)
{
  nun = nu0 + sum(n)
  ss <- nu0 * s20
  for(i in 1:m) ss = ss+sum((Y$arsenic[which(Y$food_num==i)] - theta[j])^2)
  sigma2 <- 1/rgamma(1, nun/2, ss/2)
  return(sigma2)
}
newMu <- function(m, theta, tau2, g20)
{
  v = 1/(m/tau2 + 1/g20)
  e = v *(m*mean(theta)/tau2 + mu0/g20)
  mu <- rnorm(1, e, v)
  return(mu)
}
newTau2 <- function(m, eta0, t20, theta, mu)
{
  etam = eta0 + m
  ss <- eta0*t20 + sum( (theta-mu) ^2 )
  tau2 <- 1/rgamma(1, etam/2, ss/2)
  return(tau2)
}
```
Before we go any further, we check that the MCMC model converged for all four statistics using ggplot2 (code used for $\mu$ repeated for other parameters):

```{.r fig.width=10, fig.height=1.5}
library(ggplot2)
graphdata <- data.frame(
              "Iteration"=c(1:S), "Mu"=OTH[,1], "Sigma2"=OTH[,2], "Tau2"=OTH[,3], 
              "Theta_1" = THETA[,1], "Theta_2" = THETA[,2], "Theta_3" = THETA[,3], 
              "Theta_4" = THETA[,4], "Theta_5" = THETA[,5])
              
ggplot(graphdata,aes(x=Iteration,y=Mu)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")

```
```{.r fig.width=10, fig.height=1.5, echo=FALSE}

ggplot(graphdata,aes(x=Iteration,y=Sigma2)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Tau2)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Theta_1)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Theta_2)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Theta_3)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Theta_4)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Theta_5)) +
theme_minimal(base_family = "") + geom_line(colour="wheat3")
```

We conclude from the graphs that convergence was achieved for all parameters. 

\subsection{1.3 Algorithm output}
The estimated median values and 95\% credible intervals for the parameters are as follow:

```{.r results="hide"}
  for(i in 1:length(ALL[1,])) print(round(unname(
    quantile(ALL[,i], probs=c(0.025, 0.5, 0.975))
    ),3))
```
\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati) & 2.810 & 3.569 & 4.308 \\ 
    $\theta_2$ (Non-Basmati) & 5.790 & 6.533 & 7.305 \\
    $\theta_3$ (Beverage) & 1.869 & 4.231 & 6.448 \\
    $\theta_4$ (Cakes) & 4.486 & 5.370 & 6.301 \\
    $\theta_5$ (Cereal) & 2.705 & 3.610 & 4.507 \\
    $\mu$ & 3.193 & 4.697 & 6.182 \\
    $\sigma^2$ & 5.109 & 7.027 & 10.573 \\
    $\tau^2$ & 0.693 &2.251 & 12.920 \\ \hline
  \end{tabular}
\end{center}

\subsection{1.4 Sensitivity analyses}
Evaluation of sensitivity to priors: we try three separate scenarios each tuning prior distribution of parameters:
\begin{enumerate}
  \item Large expected $\mu$ (Prior expectation of mad levels of arsenic)
  \item Large $\sigma^2$ and $\nu_0$ (High variability within products)
  \item Large $\tau^2$ and $\eta_0$ (High variability between products)
\end{enumerate}

\textit{Scenario 1}:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati)&2.706&3.488&4.265\\
    $\theta_2^2$ (Non-Basmati)&5.902&6.671&7.460\\
    $\theta_3 (Beverage)$&0.643&3.774&6.932\\
    $\theta_4 (Cakes)$&4.511&5.452&6.429\\
    $\theta_5$ (Cereal)&2.548&3.496&4.456\\
    $\mu$ &2.548&3.496&4.456\\
    $\sigma^2$ &88.793&100&111\\
    $\tau^2$ &3021.927&8427&37962\\ \hline
  \end{tabular}
\end{center}

\textit{Scenario 2}:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\% \\ \hline
    $\theta_1$ (Basmati)& 1.237 & 4.396 & 7.069\\
    $\theta_2^2$ (Non-Basmati)& 2.811 & 5.401 & 8.559 \\
    $\theta_3 (Beverage)$& 0.278 & 4.790 & 9.023 \\
    $\theta_4 (Cakes)$& 1.931 & 4.960 & 8.221\\
    $\theta_5$ (Cereal)& 1.137 & 4.531 & 7.508\\
    $\mu$ &2.42 & 4.84 & 7.19\\
    $\sigma^2$ &182 & 215& 256\\
    $\tau^2$ &0.399 & 1.876 & 22.3\\ \hline

  \end{tabular}
\end{center}

\textit{Scenario 3}:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\% \\ \hline
    $\theta_1$ (Basmati)& 2.707 & 3.478 & 4.268\\
    $\theta_2^2$ (Non-Basmati)& 5.907 & 6.674 & 7.451 \\
    $\theta_3 (Beverage)$& 0.676 & 3.739 & 6.9023 \\
    $\theta_4 (Cakes)$& 4.504 & 5.447 & 6.419\\
    $\theta_5$ (Cereal)& 2.547 & 3.494 & 4.452\\
    $\mu$ &-5.302 & 4.845 & 5.00\\
    $\sigma^2$ &5.197 & 7.327 & 11.37\\
    $\tau^2$ &223 & 288 & 384\\ \hline
  \end{tabular}
\end{center}


We observe that excessively large prior expectations of $\mu$ will drive up estimates of the within- and between-group variances but will have little effect on the magnitude of the estimates of within-groupmean estimates (although the precision may be negatively affected for groups with realtively few observations). A large prior within-sample variance will bring posterior within-group means closer to $\mu$, as could be expected since the posterior estimates need to become more conservative. Increasing prior between-sample variance appears to drive up uncertainty on $\mu$ and bring it closer to $0$, without however having a notable impact on the rest of the model.

\subsection{1.5 Results presentation}

Non-Basmati rice had the highest arsenic concentration, at an estimated 6.7 mcg/serving. Rice cakes came second, at 5.4 mcg/serving, and non-Basmati and rice cereal had comparatively low amounts, slightly below 3.5 mcg/serving. There lacked data to reliably evaluate arsenic concentration in rice beverages, whose 3.8 mcg/serving estimate was particulary imprecise (95\% CI=0.64, 6.93). Posterior median estimates and observed mean concentrations of arsenic are presented by product type in the following graph. Markers are $\theta$ estimates with $95\%$ credible interval lines; horizontal lines are the median estimate of $\mu$ (solid) and corresponding $95\%$ credibal interval (dashed).

```{.r fig.width=10, fig.height=5, warning=FALSE}
qmat=apply(THETA[,1:5],2,quantile,probs=c(0.025,.5,0.975))
mu_ci = quantile(OTH[,1], probs=(c(0.025, 0.5, 0.975)))
res <- data.frame("Rice"=c("Non-Basmati", "Basmati", "Beverages", "Cakes", "Cereal"),
                         "l95"=qmat[1,], "median"=qmat[2,], "u95"=qmat[3,], "mean"=ybar)
g <- ggplot(res, aes(x = Rice, group=Rice, colour=Rice)) + 
  labs(x="Rice Products", y="Arsenic concentration, mcg/serving") + 
  theme(legend.position="none", panel.background =element_rect(colour = "black")) +
  scale_y_continuous(breaks=seq(0, 7.5, 1)) +
  geom_hline(aes(yintercept=c(mu_ci[2])), size=0.7) +
  geom_hline(aes(yintercept=c(mu_ci[1])), linetype="dashed") +
  geom_hline(aes(yintercept=c(mu_ci[3])), linetype="dashed") +
  geom_errorbar(aes(ymin=l95, ymax=u95), width=.3, size=0.8) +
  geom_point(aes(y=median), fill="white", shape=21, size=5)  +
  geom_point(aes(y=mean), fill="red", shape=21, size=3)
g
```
\newpage \section{2. Parameter-expanded specification of the hierarchical model}
\subsection{2.1 Model specification}
Under this model specification, instead of group means we are interested in differences between groups and the population average $\mu$, which is given by $\eta_j$ for the group $j$, so that (under the prior belief that all groups will have equal mean):
\begin{align*}
  y_{ij} \sim& \, \mathcal{N}\left( \mu + \xi\eta_j, \sigma_y^2 \right)\\
  \eta_i \sim& \, \mathcal{N}\left( 0, \sigma_\eta^2 \right)
\end{align*}
Word's on the street that well-behaved conditionally-conjugate specifications for the distributions of $\xi$ and $\sigma_\eta^2$ are:
\begin{align*}
  \xi \sim& \, \mathcal{N}\left( 0, 1 \right) \\
  1/\sigma^2_\eta \sim& \, \text{gamma}\left( \frac{\omega_0}{2}, \frac{\omega_0\sigma^2_{\eta0}}{2} \right) \\
  1/\sigma^2_y \sim& \, \text{gamma}\left( \frac{\nu_0}{2}, \frac{\nu_0\sigma^2_{y0}}{2}\right)
\end{align*}
The prior distribution of the population mean is still $\mu \sim \, \mathcal{N}\left( \mu_0, \gamma_0^2\right)$. We set out to find full conditionals:
Reparametrizing the  full conditionals in exercise 1 easily yields what we need: \newline

Note that each of the $N$ observation in the data supports, $\mu = y_{ij} - xi\eta_j$ given known $\xi$ and $\eta_j$'s, so that, summing over this expression and weighting it against the prior yields:
\begin{flalign*}
p\left( \mu, \sigma^2_y, \sigma^2_\eta, \xi, \eta_1,\ldots,\eta_m \, | \, \mathbf{y}\right) \alpha& \; \mathcal{N} \left( \frac{\frac{\sum\limits_j\sum\limits_i y_{ij} - \xi\eta_j}{\sigma^2_y} + \frac{\mu_0}{\gamma_0^2}}{\frac{N}{\sigma^2_y}+\frac{1}{\gamma_0^2}},\frac{1}{\frac{N}{\sigma^2_y}+\frac{1}{\gamma_0^2}} \right) &
\end{flalign*}
Posterior on the variances can be similarly rewritten. The sum of squares for $\mathbf{y}$ given $\mu, \eta_j$ and $\xi$ is, of course, $\sum\limits_j\sum\limits_i y_{ij} - \mu - \xi\eta_j$, and the sum of squares of $\boldsymbol{\eta}$ given its $\mathbb{E}=0$ is simply $\sum\limits_j\eta_j^2$ 
\begin{flalign*}
p\left( 1/\sigma^2_y \, | \, \mathbf{y}, \mu, \sigma^2_\eta, \xi, \eta_1,\ldots,\eta_m \right) \alpha& \; \text{gamma }\left( \frac{\nu_0 + N}{2}, \frac{1}{2}\left( \nu_0\sigma_{y0}^2 + \sum\limits_j\sum\limits_i \left(y_{ij} - \mu - \xi\eta_j \right)^2 \right) \right) & \\
p\left( 1/\sigma^2_\eta \, | \, \mathbf{y}, \mu, \sigma^2_y, \xi, \eta_1,\ldots,\eta_m \right) \alpha& \; \text{gamma }\left( \frac{\omega_0 + m}{2}, \frac{1}{2}\left( \omega_0\sigma_{\eta_0}^2 + \sum\limits_j\eta_j^2 \right) \right)  &
\end{flalign*}
Getting full conditionals on $\boldsymbol{\eta}$ and $\xi$ can be likewise achieved by manipulating $y_{ij} = \mu + \eta_j\xi$.
\begin{flalign*}
  p\left( \eta_j \, | \, \mathbf{y}, \mu, \sigma^2_y, \sigma^2_\eta, \xi \right) \alpha& \; \mathcal{N}\left( \frac{\frac{\xi\sum\limits_iy_{ij}-\mu}{\sigma^2_y}}{\frac{n_j\xi^2}{\sigma^2_y}+\frac{1}{\sigma^2_\eta}}, \frac{1}{{\frac{n_j\xi^2}{\sigma^2_y}+\frac{1}{\sigma^2_\eta}}}\right) & 
\end{flalign*}
and
\begin{flalign*}
  p\left( \xi \, | \, \mathbf{y}, \mu, \sigma^2_y, \sigma^2_\eta, \eta_1,\ldots,\eta_m \right) \alpha& \; \mathcal{N} \left( \frac{\frac{\sum\limits_j\eta_j\sum\limits_i \left( y_{ij} - \mu \right)}{\sigma^2_y}}{\frac{\sum\limits_j n_j \eta_j^2}{\sigma^2_y}+1}, \frac{1}{\frac{\sum\limits_j n_j \eta_j^2}{\sigma^2_y}+1}\right) &
\end{flalign*}
\subsection{2.2 Analyses}
Similarly to exercise 1, we pick the priors $\sigma^2_{y0} = 10, \nu_0 = 1, \omega_0 = 1, \sigma_{0\eta}^2 = 10$ and $\gamma_0^2 = 10$, and proceed with Gibbs sampling:

We read the data
```{.r}
library(foreign)
library(hdrcde)
data <- read.dta(file="arsenicrice2.dta")
Y <- read.dta(file="arsenicrice2.dta")

```

We set prior values

```{.r}
n <-  nrow(Y)
nu0 <- 1; omega0 <- 1; 
s2_eta0 <- 10; s2_y0 <- 10
mu0 <- mean(Y$arsenic); 
g20 <- var(Y$arsenic)
```

We setup the MCMC

```{.r}
#Setup starting values 
m <- length(unique(Y$food_num)) #number of groups
n <- sv_y <- ybar <- rep(NA,m) #create empty vectors for group descriptions
for (i in 1:m) 
{
  n[i] <- sum(Y$food_num==i)
  sv_y[i] <- var(Y$arsenic[which(Y$food_num==i)])
  ybar[i] <-  mean(Y$arsenic[which(Y$food_num==i)])
}
eta <- ybar - mean(Y$arsenic); s2_y <- mean(sv_y)
mu <- mean(Y$arsenic); s2_eta <- var(eta)
xi <- 0

#Setup MCMC
set.seed(0808)
S <- 10000
THETA <- matrix(nrow=S, ncol=m)
RES <- matrix(nrow=S, ncol=4)
ALL <- matrix(nrow=S, ncol=4+m)

#Setup MCMC
set.seed(0808)
S <- 10000
ETA <- matrix(nrow=S, ncol=m)
RES <- matrix(nrow=S, ncol=4)
ALL <- matrix(nrow=S, ncol=4+m)
```

Our updating functions correspond to the full conditionals derived above:

```{.r}
#FUNCTIONS
newS2eta <- function(m, s2_eta0, eta, omega0)
{
  ss <- s2_eta0*omega0 + sum( eta^2 )
  s2_eta <- 1/rgamma(1, shape=((omega0 + m)/2), scale=(ss/2))
  return(s2_eta)
}
newS2y <- function(nu0, n, s2_y0, y, m, eta, xi, mu)
{
  nun = nu0 + sum(n)
  ss = sum((y - mu - xi*rep(eta, times=n))^2) + nu0*s2_y0
  s2_y <- 1/rgamma(1, shape=(nun/2), scale=(ss/2))
  return(s2_y)
}
newMu <- function(y, xi, eta, s2_y, g20, mu0, n)
{
  v = ( sum(n)/s2_y + 1/g20 )
  ss = sum(y-rep(eta,times=n)*xi)/s2_y + mu0/g20
  e = (ss/s2_y + mu0/g20)
  mu = rnorm(1, ss/v, sqrt(1/v))
  return(mu)
}
newEta <- function(xi, mu, ybar, n, s2_y, s2_eta)
{
  v = 1/(n*xi^2/s2_y + 1/s2_eta)
  e = v*xi*(n*ybar - mu*n)/s2_y
  eta = rnorm(m, e, sqrt(v))
  return(eta)
}
newXi <- function(eta, m, y, n, mu, s2_y)
{
  v = (sum(n*eta^2)/s2_y + 1)
  e = sum((y-mu)*rep(eta,times=n))/s2_y
  xi = rnorm(1, e/v, sqrt(1/v))
  return(xi)
}
```

We run the MCMC algorithm:
```{.r}
#RUN MCMC
for(i in 1:S)
{
  #Get new values for parameters
  eta <- newEta(xi, mu, ybar, n, s2_y, s2_eta)
  mu <- newMu(Y$arsenic, xi, eta, s2_y, g20, mu0, n)
  
  s2_eta <- newS2eta(m, s2_eta0, eta, omega0)
  s2_y <- newS2y(nu0, n, s2_y0, Y$arsenic, m, eta, xi, mu)
  
  xi <- newXi(eta, m, Y$arsenic, n, mu, s2_y)
  #Store in chain
  ETA[i,] <- eta
  RES[i,] <- c(mu,s2_y, s2_eta, xi)
  ALL[i,] <- c(eta, mu, xi, s2_eta, s2_y)
}
```

Again, before we get too psyched about results, we check MCMC convergence criteria (code not shown, see above):

```{.r fig.width=10, fig.height=1.5, echo=FALSE}
graphdata <- data.frame(
  "Iteration"=c(1:S), "Mu"=RES[,1], "s2_y"=RES[,2], "s2_eta"=RES[,3], "xi"=RES[,4],
  "Eta_1" = ETA[,1], "Eta_2" = ETA[,2], "Eta_3" = ETA[,3], 
  "Eta_4" = ETA[,4], "Eta_5" = ETA[,5])
ggplot(graphdata,aes(x=Iteration,y=Mu)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=s2_y)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=s2_eta)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=xi)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Eta_1)) +
  theme_minimal(base_family = "") + geom_point(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Eta_2)) +
  theme_minimal(base_family = "") + geom_point(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Eta_3)) +
  theme_minimal(base_family = "") + geom_point(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Eta_4)) +
  theme_minimal(base_family = "") + geom_point(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Eta_5)) +
  theme_minimal(base_family = "") + geom_point(colour="wheat3")
```

Obviously, something is wrong with the $\eta$ parameters, which could make sense since when the estimate of $\xi$ crosses zero, the $\eta$ parameters get updated on the other side of $0$ as well. Taking the absolute value of $\xi$ reassures us that the parameter space that is actually searched isn't terribad. We also check the convergence of $\mu + \xi\eta$, which is the mean concentration of arsenic in each group, and ultimately interests us. Reproduced below are the graphs for $|\eta_1|, |\eta_2|$, and $\boldsymbol{\theta}$

```{.r fig.width=10, fig.height=1.5, echo=FALSE}
ggplot(graphdata,aes(x=Iteration,y=abs(Eta_1))) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=abs(Eta_2))) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
  
  ggplot(graphdata,aes(x=Iteration,y=Mu+Eta_1*xi)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Mu+Eta_2*xi)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Mu+Eta_3*xi)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=mu+Eta_4*xi)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=mu+Eta_5*xi)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
```
We are fairly satisfied with the outlook on the convergence of our $\theta$'s, although obviously the wacky errors we'll get mean the credible intervals we get for $\theta$'s will be meaningless.

\begin{figure}[h]
  \centering \includegraphics[width=0.7\textwidth]{hadeswrong}
  \caption{I know, Hades isn't pleased either}
\end{figure}
\subsection{2.3 Algorithm output}
We provide the median estimate and 95\% credible interval for the $\theta$ parameters in this expanded hierarchical specification model:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati) & 3.610 & 3.619 & 3.639 \\ 
    $\theta_2$ (Non-Basmati) & 6.420 & 6.454 & 6.503 \\
    $\theta_3$ (Beverage) & 4.072 & 4.361 & 4.544 \\
    $\theta_4$ (Cakes) & 5.308 & 5.331 & 5.345 \\
    $\theta_5$ (Cereal) & 3.649 & 3.670 & 3.747 \\ \hline
  \end{tabular}
\end{center}

\subsection{2.4 Sensitivity analyses}
We repeat the sensitivity analyses of section 1.4:
\begin{enumerate}
  \item Large expected $\mu$ (Prior expectation of mad levels of arsenic)
  \item Large $\sigma^2$ and $\nu_0$ (High variability within products)
  \item Large $\tau^2$ and $\eta_0$ (High variability between products)
\end{enumerate}

\textit{Scenario 1}:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati) & 3.610 & 3.619 & 3.639 \\ 
    $\theta_2$ (Non-Basmati) & 6.421 & 6.454 & 6.503 \\
    $\theta_3$ (Beverage) & 4.072 & 4.361 & 4.544 \\
    $\theta_4$ (Cakes) & 5.308 & 5.332 & 5.345 \\
    $\theta_5$ (Cereal) & 3.649 & 3.670 & 3.747 \\ \hline
  \end{tabular}
\end{center}

\textit{Scenario 2}:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati) & 3.610 & 3.619 & 3.639 \\ 
    $\theta_2$ (Non-Basmati) & 6.420 & 6.454 & 6.503 \\
    $\theta_3$ (Beverage) & 4.072 & 4.361 & 4.544 \\
    $\theta_4$ (Cakes) & 5.308 & 5.331 & 5.345 \\
    $\theta_5$ (Cereal) & 3.649 & 3.670 & 3.747 \\ \hline
  \end{tabular}
\end{center}

\textit{Scenario 3}:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati) & 3.610 & 3.619 & 3.639 \\ 
    $\theta_2$ (Non-Basmati) & 6.420 & 6.454 & 6.503 \\
    $\theta_3$ (Beverage) & 4.072 & 4.361 & 4.544 \\
    $\theta_4$ (Cakes) & 5.308 & 5.331 & 5.345 \\
    $\theta_5$ (Cereal) & 3.649 & 3.670 & 3.747 \\ \hline
  \end{tabular}
\end{center}

We observe that outrageous standard errors will prevent any meaningful sensitivity analyses.

\subsection{2.5 Results presentation}

Based on our analyses, and under the assumption we did nothing incorrect, we are nearly positive that the type of rice with the greatest arsenic concentration is non-basmati rice, at 6.420 mcg/serving, followed by cake rice products, at 5.308 mcg/serving. Basmati and cereal products both neared 6.6mcg of arsening  per serving, beverage products, which should have constituted an imprecise estimate (but did not), had an estimated arsenic concentration at 6.454 mcg/serving (95\% CI=4.361,4.544).

Posterior median estimates and observed mean concentrations of arsenic are presented by product type in the following graph. Markers are $\theta$ estimates with $95\%$ credible interval lines; horizontal lines are the median estimate of $\mu$ (solid) and corresponding $95\%$ credibal interval (dashed, like my hopes and dreams).

```{.r fig.width=10, fig.height=5, warning=FALSE, echo=FALSE}
qmat=apply(ETA*RES[,4]+RES[,1],2,quantile,probs=c(0.025,.5,0.975))
res <- data.frame("Rice"=c("Non-Basmati", "Basmati", "Beverages", "Cakes", "Cereal"),
                  "l95"=qmat[1,], "median"=qmat[2,], "u95"=qmat[3,], "mean"=ybar)
g <- ggplot(res, aes(x = Rice, group=Rice, colour=Rice)) + 
  labs(x="Rice Products", y="Arsenic concentration, mcg/serving") + 
  theme(legend.position="none", panel.background =element_rect(colour = "black")) +
  scale_y_continuous(breaks=seq(0, 7.5, 1)) +
  geom_hline(aes(yintercept=c(mu_ci[2])), size=0.7) +
  geom_hline(aes(yintercept=c(mu_ci[1])), linetype="dashed") +
  geom_hline(aes(yintercept=c(mu_ci[3])), linetype="dashed") +
  geom_errorbar(aes(ymin=l95, ymax=u95), width=.3, size=0.8) +
  geom_point(aes(y=median), fill="white", shape=21, size=5)  +
  geom_point(aes(y=mean), fill="red", shape=21, size=3)
g
```


\section{3. Conditionally-conjugate specification of the hierarchical model with group-specific variances}
\subsection{3.1 Model specification}
This model is similar to that laid out in section 1.1, with the exceptions that variance is allowed to vary between groups, with group-specific variances following a conjugate inverse-gamma distribution so that (from Hoff):
\begin{align*}
\left\{ \theta_j \, \left| \, \sigma^2, y_{j,1},\ldots,y_{j,n} \right. \right\} \sim& \, \mathcal{N}\left( \frac{n_j\bar{y}_j/\sigma^2+\mu/\tau^2}{n_j/\sigma^2 + 1/\tau^2}, \left[ n_j/\sigma^2+1/\tau^2\right]^{-1} \right) \\
	\left\{1/\sigma^2_j \, \left| \, \boldsymbol{\theta}, y_1, \ldots, y_n \right. \right\} \sim& \, \text{gamma}\left( \frac{1}{2}\left[ \nu_0 + n_j \right], \frac{1}{2}\left(\nu_0\sigma_0^2 + \sum\limits_{i=1}^{n_j} \left(y_{i,j} -\theta_j \right)^2 \right)\right) \\
	\left\{ \sigma^2_0 \left| \boldsymbol{\sigma}, \nu_0 \right. \right\} \sim& \, \text{gamma} \left( a + \frac{1}{2}m\nu_0, b + \frac{1}{2}\sum\limits_j\left( 1/\sigma^2_j \right) \right)
	\intertext{and}
	\left\{\nu_0 \left| \boldsymbol{\sigma} \right. \right\} \sim& \left( \frac{\left( \nu0\sigma_0^2/2\right)^{\nu_0/2}}{\Gamma\left( \nu_0/2\right)} \right)^m \left( \prod\limits_j\frac{1}{\sigma^2_j} \right)^{\nu_0/2-1} \times \exp\left\{ -\nu_0 \left( \alpha + \frac{1}{2} \sigma^2_0 \sum\limits_j \left( \frac{1}{\sigma^2_j}\right)\right) \right\}
\end{align*}
\subsection{3.2 Analyses}
We keep the same priors for hyperparameters pertaining to $\mu$ and $\theta$, and additionally specify $\alpha = 1, b = 3, a = 1$ for the prior distributions on $\nu_0$ and $\sigma^2_0$. Code to run analyses is as follows, please refer to the earlier sections for added description.

```{.r}
data <- read.dta(file="arsenicrice2.dta")
Y <- read.dta(file="arsenicrice2.dta")

#We set weakly informative prior values
n <-  nrow(Y)
eta0 <- 1; t20 <- 3
mu0 <- mean(Y$arsenic) 
g20  <- var(Y$arsenic)
a = 1; b = 3; alpha = 1

# mu0 <- 100 #scenario 1
# s20 <- 100*s20; nu0 <- 100 #scenario 2
# t20 <- 100*t20; eta0 <- 100 #scenario 3
#Setup starting values 

m <- length(unique(Y$food_num)) #number of groups
n <- s2 <- ybar <- rep(NA,m) 
for (i in 1:m) 
{
  n[i] <- sum(Y$food_num==i)
  s2[i] <- var(Y$arsenic[which(Y$food_num==i)])
  ybar[i] <- mean(Y$arsenic[which(Y$food_num==i)])
}
theta <- ybar; s2_0 <- mean(sv)
mu <- mean(theta); tau2 <- var(theta)
nu0 <- 1

#Setup MCMC
set.seed(0808)
S <- 10000
THETA <- S2 <- matrix(nrow=S, ncol=m)
RES <- matrix(nrow=S, ncol=4)
ALL <- matrix(nrow=S, ncol=4+2*m)
NUMAX <- 10000

### Functions sampling from posteriors
newTheta <- function(n, ybar, s2, tau2, mu)
{
  v = 1/(n/s2 +1/tau2)
  e = v * (ybar*n/s2 + mu/tau2)
  new <- rnorm(length(n), e, sqrt(v))
  return(new)
}
newSigma2 <- function(m, n, nu0, s20, theta, y)
{
  nun = n + nu0
  ss <- nu0 * s20 + sum((y - rep(theta, times=n))^2) 
  sigma2 <- 1/rgamma(m, nun/2, ss/2)
  return(sigma2)
}
newMu <- function(m, theta, tau2, g20)
{
  v = 1/(m/tau2 + 1/g20)
  e = v *(m*mean(theta)/tau2 + mu0/g20)
  mu <- rnorm(1, e, sqrt(v))
  return(mu)
}
newTau2 <- function(m, eta0, t20, theta, mu)
{
  etam = eta0 + m
  ss = eta0*t20 + sum( (theta-mu) ^2 )
  tau2 <- 1/rgamma(1, etam/2, ss/2)
  return(tau2)
}
newS20 <- function(m, nu0, s2, a, b)
{
  L = a + 0.5*m*nu0
  R = b + 0.5*sum(1/s2^2)
  s20 <- rgamma(1, L, R)
}

### Run MCMC
for(i in 1:S)
{
  #Get new values for parameters
  theta <- newTheta(n, ybar, s2, tau2, mu)
  s2 <- newSigma2(m, n, nu0, s20, theta, Y$arsenic)
  mu <- newMu(m, theta, tau2, g20)
  tau2 <- newTau2(m, eta0, t20, theta, mu)
  s20 <- newS20(m, nu0, s2, a, b)
  
  x <- 1:NUMAX
    lpnu0<-m*(.5*x*log(s20*x/2)-lgamma(x/2))+
      (x/2-1)*sum(log(1/s2))+
      -x*(alpha+.5*s20*sum(1/s2))
  nu0<-sample(x,1,prob=exp(lpnu0-max(lpnu0)))
  
  
  #Store in chain
  THETA[i,] <- theta
  S2[i,] <- s2
  RES[i,] <- c(mu,tau2, nu0, s20)
  ALL[i,] <- c(theta,s2,mu,tau2, nu0, s20)
}
```

Trace plots for the parameters follow:

```{.r fig.width=10, fig.height=1.5, echo=FALSE}
graphdata <- data.frame("Iteration"=c(1:S), "Mu"=RES[,1], "Tau2"=RES[,2], "Nu_0"=RES[,3], "Sigma2_0"=RES[,4], 
                        "Theta_1" = THETA[,1], "Theta_2" = THETA[,2], "Theta_3" = THETA[,3],
                        "Theta_4" = THETA[,4], "Theta_5" = THETA[,5], 
                        "Sigma_1" = S2[,1], "Sigma_2" = S2[,2], "Sigma_3" = S2[,3],
                        "Sigma_4" = S2[,4], "Sigma_5" = S2[,5])

ggplot(graphdata,aes(x=Iteration,y=Mu)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Tau2)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Nu_0)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Sigma2_0)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Theta_1)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Theta_2)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Theta_3)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Theta_4)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Theta_5)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

ggplot(graphdata,aes(x=Iteration,y=Sigma_1)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Sigma_2)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Sigma_3)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Sigma_4)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")
ggplot(graphdata,aes(x=Iteration,y=Sigma_5)) +
  theme_minimal(base_family = "") + geom_line(colour="wheat3")

```

Other than $\sigma^2_3$ and $\nu_0$, everything looks happy and converged. $\sigma_3$ probably has a large range due to the small number of non-missing data on rice beverages.

\begin{figure}[h]
  \caption{Other than those two, we're good}
  \centering \includegraphics[width=0.9\textwidth]{hades_mad}
\end{figure}

\subsection{3.3 Algorithm output}

Estimates for parameter medians and their 95\% credible intervals are shown in the following table:

\begin{center}
  \begin{tabular}{l c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati) & 2.237 & 3.826 & 5.361 \\ 
    $\theta_2$ (Non-Basmati) & 4.713 & 6.240 & 7.868 \\
    $\theta_3$ (Beverage) & 0.394 & 4.841 & 9.097 \\
    $\theta_4$ (Cakes) & 3.108 & 5.200 & 7.355 \\
    $\theta_5$ (Cereal) & 1.827 & 4.042 & 6.122 \\ \hline
    $\sigma_1$ (Basmati) & 18.545 & 33.132 & 61.644 \\ 
    $\sigma_2$ (Non-Basmati) & 18.650 & 33.084 & 60.759 \\
    $\sigma_3$ (Beverage) & 131.361 & 489.618 & 3600.339 \\
    $\sigma_4$ (Cakes) & 27.028 & 50.380 & 100.924 \\
    $\sigma_5$ (Cereal) & 26.807 & 50.369 & 101.083 \\ \hline
    $\mu$ (Basmati) & 2.926 & 4.841 & 6.846 \\ 
    $\tau_2$ (Non-Basmati) & 0.573 & 2.448 & 16.913 \\
    $\nu_0$ (Beverage) & 1 & 1 & 1 \\
    $\sigma2_0$ (Beverage) & 0.277 & 1.060 & 2.712 \\ \hline
  \end{tabular}
\end{center} 

\subsection{3.4 Sensitivity analyses}

We repeat the sensitivity analyses of section 1.4:
\begin{enumerate}
  \item Large expected $\mu$ (Prior expectation of mad levels of arsenic)
  \item Large $b$ and $\alpha$ (High variability within products)
  \item Large $\tau^2$ and $\eta_0$ (High variability between products)
\end{enumerate}

\textit{Scenario 1}:

\begin{center}
  \begin{tabular}{l c c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\%  \\ \hline
    $\theta_1$ (Basmati) & 0.913 & 3.505 & 6.207 & Yes\\ 
    $\theta_2$ (Non-Basmati) & 4.062 & 6.686 & 9.437 & Yes \\
    $\theta_3$ (Beverage) & -32.754 & 8.074 & 71.878 & Yes \\
    $\theta_4$ (Cakes) & 1.528 & 5.485 & 9.821 & Yes \\
    $\theta_5$ (Cereal) & -0.448 & 3.504 & 7.611 & Yes \\ \hline
    $\sigma_1$ (Basmati) & 24.947 & 58.269 & 404.571 & Yes \\ 
    $\sigma_2$ (Non-Basmati) & 25.104 & 57.722 & 404.574 & Yes  \\
    $\sigma_3$ (Beverage) & 194.759 & 931.623 & 12209.972 & Yes  \\
    $\sigma_4$ (Cakes) & 36.606 & 89.011 & 626.494 & Yes  \\
    $\sigma_5$ (Cereal) & 35.694 & 89.044 & 628.468 & Yes  \\ \hline
    $\mu$ (Basmati) & 95.120  & 99.723 & 104.288 & Yes  \\ 
    $\tau_2$ (Non-Basmati) & 3033.227 & 8299.341 & 38038.052 & Yes!!  \\
    $\nu_0$ (Beverage) & 1 & 1 & 1 & No \\
    $\sigma2_0$ (Beverage) & 0.277 & 1.060 & 2.712  & Yes \\ \hline
  \end{tabular}
\end{center}

\textit{Scenario 2}:

\begin{center}
  \begin{tabular}{l c c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\% & Big change? \\ \hline
    $\theta_1$ (Basmati) & 1.408 & 3.479 & 5.583 & Meh\\ 
    $\theta_2$ (Non-Basmati) & 4.618 & 6.665 & 8.690 & Meh\\
    $\theta_3$ (Beverage) & -19.383 & 4.385 & 27.663 & Meh\\
    $\theta_4$ (Cakes) & 2.331 & 5.453 & 8.664 & Meh \\
    $\theta_5$ (Cereal) & 0.273 & 3.465 & 6.660 & Yes\\ \hline
    $\sigma_1$ (Basmati) & 23.249 & 47.513 & 107.471 & Meh \\ 
    $\sigma_2$ (Non-Basmati) & 23.565 & 47.531 & 106.423 & Meh \\
    $\sigma_3$ (Beverage) &  173.775 & 706.454 & 5455.201 & Meh \\
    $\sigma_4$ (Cakes) & 34.267 & 72.325 & 173.977 & Meh \\
    $\sigma_5$ (Cereal) & 33.941 & 72.486 & 174.818 & Meh \\ \hline
    $\mu$ (Basmati) & 0.456 & 4.872 & 9.262 & Yes \\ 
    $\tau_2$ (Non-Basmati) & 222.934 & 288.999 & 384.448 & Yes!! \\
    $\nu_0$ (Beverage) & 1 & 1 & 1 & No\\
    $\sigma2_0$ (Beverage) & 0.279 & 1.058 & 2.712 & Yes\\ \hline
  \end{tabular}
\end{center}

\textit{Scenario 3}:

\begin{center}
  \begin{tabular}{l c c c c}
  \hline Parameter & Credible Lower 95\% & Median & Credible Upper 95\% & Big change? \\ \hline
    $\theta_1$ (Basmati) & 1.408 & 3.479 & 5.583 & No\\ 
    $\theta_2$ (Non-Basmati) & 4.674 & 6.234 & 7.817 & No\\
    $\theta_3$ (Beverage) & 0.490 & 4.814 & 9.144 & No\\
    $\theta_4$ (Cakes) & 3.126 & 5.194 & 7.311 & No \\
    $\theta_5$ (Cereal) & 1.864 & 4.039 & 6.094 & No\\ \hline
    $\sigma_1$ (Basmati) & 18.609 & 33.008 & 60.612 & No \\ 
    $\sigma_2$ (Non-Basmati) & 18.807 & 33.083 & 61.010 & No \\
    $\sigma_3$ (Beverage) &  131.637 & 487.250 & 3367.378 & No \\
    $\sigma_4$ (Cakes) & 26.960 & 50.231 & 98.334 & No \\
    $\sigma_5$ (Cereal) & 26.904 & 50.614 & 100.647 & No \\ \hline
    $\mu$ (Basmati) & 2.926 & 4.828 & 6.741 & No \\ 
    $\tau_2$ (Non-Basmati) & 0.584 & 2.448 & 16.443 & No \\
    $\nu_0$ (Beverage) & 1 & 1 & 1 & No\\
    $\sigma2_0$ (Beverage) & 0.121 & 0.163 & 0.214 & Yes\\ \hline
  \end{tabular}
\end{center}

Results from these sensitivity analyses are analogous to those from section 1.4; we observe that increasing $\mu_0$ increases shrinking towards $\mu$ and the error on $\theta$'s; increasing $a$ and $b$ increases the error on $\theta$'s and the estimates of $\sigma$'s; increasing $\tau^2_0$ and $\eta_0$ enlarges the credible interval around estimates of $\sigma$'s, along with, obviously, $\tau^2_0$ itself. 

\subsection{3.5 Presentation of results}

```{.r fig.width=10, fig.height=5, warning=FALSE, echo=FALSE}
qmat=apply(THETA,2,quantile,probs=c(0.025,0.5,0.975))
mu_ci = quantile(RES[,1], probs=(c(0.025, 0.5, 0.975)))
res <- data.frame("Rice"=c("Non-Basmati", "Basmati", "Beverages", "Cakes", "Cereal"),
                  "l95"=qmat[1,], "median"=qmat[2,], "u95"=qmat[3,], "mean"=ybar)
g <- ggplot(res, aes(x = Rice, group=Rice, colour=Rice)) + 
  labs(x="Rice Products", y="Arsenic concentration, mcg/serving") + 
  theme(legend.position="none", panel.background =element_rect(colour = "black")) +
  scale_y_continuous(breaks=seq(0, 7.5, 1)) +
  geom_hline(aes(yintercept=c(mu_ci[2])), size=0.7) +
  geom_hline(aes(yintercept=c(mu_ci[1])), linetype="dashed") +
  geom_hline(aes(yintercept=c(mu_ci[3])), linetype="dashed") +
  geom_errorbar(aes(ymin=l95, ymax=u95), width=.3, size=0.8) +
  geom_point(aes(y=median), fill="white", shape=21, size=5)  +
  geom_point(aes(y=mean), fill="red", shape=21, size=3)
g
```
